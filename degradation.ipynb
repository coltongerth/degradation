{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOW0TngKffxhn3VOy8uuCgL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coltongerth/degradation/blob/main/degradation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dT7SoAoSugrt"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install affine==2.3.1\n",
        "!pip install attrs==22.2.0\n",
        "!pip install black==22.12.0\n",
        "!pip install bounded-pool-executor==0.0.3\n",
        "!pip install certifi==2022.12.7\n",
        "!pip install click==8.1.3\n",
        "!pip install click-plugins==1.1.1\n",
        "!pip install cligj==0.7.2\n",
        "!pip install mypy-extensions==0.4.3\n",
        "!pip install numpy==1.24.1\n",
        "!pip install packaging==23.0\n",
        "!pip install pandas==1.5.2\n",
        "!pip install pathspec==0.10.3\n",
        "!pip install patsy==0.5.3\n",
        "!pip install platformdirs==2.6.2\n",
        "!pip install pyparsing==3.0.9\n",
        "!pip install python-dateutil==2.8.2\n",
        "!pip install pytz==2022.7\n",
        "!pip install rasterio==1.3.4\n",
        "!pip install scipy==1.10.0\n",
        "!pip install six==1.16.0\n",
        "!pip install snuggs==1.4.7\n",
        "!pip install statsmodels==0.13.5\n",
        "!pip install tomli==2.0.1\n",
        "!pip install git+https://github.com/lankston-consulting/lcutils"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zonal Statistics and Degradation class instantiation."
      ],
      "metadata": {
        "id": "echHXxj-2qtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent.futures\n",
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "import rasterio\n",
        "import asyncio\n",
        "import warnings\n",
        "from statsmodels.regression.linear_model import OLS, GLSAR\n",
        "from scipy import stats as st\n",
        "from datetime import datetime\n",
        "from bounded_pool_executor import BoundedProcessPoolExecutor\n",
        "from dotenv import load_dotenv\n",
        "from lcutils import gcs, eet\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "# this one for users with access to rpms\n",
        "# path_to_credentials = \"gdrive/Shareddrives/LCLLC/fuelcast-storage-credentials.json\"\n",
        "path_to_credentials = \"gdrive/MyDrive/fuelcast-storage-credentials.json\"\n",
        "gch = gcs.GcsTools(use_service_account={\"keyfile\": path_to_credentials})\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "nodata = -3.4e38\n",
        "\n",
        "\n",
        "class Stat(object):\n",
        "    def __init__(self, zone):\n",
        "        self.zone = zone\n",
        "        self.mean = 0\n",
        "        self.std = 0\n",
        "        self.n = 0\n",
        "        self.data = list()\n",
        "\n",
        "    def add_data(self, data):\n",
        "        self.data.append(data)\n",
        "\n",
        "    def calc_stats(self):\n",
        "        if self.data:\n",
        "            self.data = np.array(self.data)\n",
        "            self.data = np.ma.masked_where(self.data < 0, self.data)\n",
        "            self.mean = np.ma.mean(self.data, axis=0)\n",
        "            self.std = np.ma.std(self.data, axis=0)\n",
        "            self.n = np.ma.count(self.data, axis=0)\n",
        "            self.n = np.ma.masked_where(\n",
        "                self.n <= 0, self.n\n",
        "            )  # n of 0 screws up combining groups later\n",
        "\n",
        "            # Clean up the object to reduce memory footprint\n",
        "            del self.data\n",
        "\n",
        "\n",
        "class StatAccumulator(object):\n",
        "    def __init__(self, update_size=500):\n",
        "        # .statistics will be a collection keyed by zone that references a list of Stat objects. As data is collected\n",
        "        # and the Stat object gets to a determined size, statistics will be calculated and the data will be deleted.\n",
        "        # New data will go in the accumulator as a new stat object. Rinse and repeat.\n",
        "        self.statistics = dict()\n",
        "        self._update_size = update_size\n",
        "        self.merged_stats = dict()\n",
        "\n",
        "    def update(self, zone, new_stats, force=False):\n",
        "        if zone not in self.statistics:\n",
        "            self.statistics[zone] = list()\n",
        "            self.statistics[zone].append(new_stats)\n",
        "        else:\n",
        "            stats_col = self.statistics[zone]\n",
        "            old_stats = stats_col[-1]  # Get the latest stat collection\n",
        "\n",
        "            # If the latest record has over x records, create a new object\n",
        "            if len(old_stats.data) > self._update_size or force:\n",
        "                old_stats.calc_stats()  # Clean up the memory footprint\n",
        "                self.statistics[zone].append(new_stats)\n",
        "            else:\n",
        "                [old_stats.data.append(d) for d in new_stats.data]\n",
        "        return\n",
        "\n",
        "    def update_cochrane(self, zone, new_stats):\n",
        "        if zone not in self.statistics:\n",
        "            self.statistics[zone] = new_stats\n",
        "        else:\n",
        "            old_stats = self.statistics[zone]\n",
        "\n",
        "            tn = old_stats.n + new_stats.n\n",
        "            tmean = (\n",
        "                np.ma.add(old_stats.n * old_stats.mean, new_stats.n * new_stats.mean)\n",
        "                / tn\n",
        "            )\n",
        "\n",
        "            # tsd = np.ma.sqrt(((old_stats.n-1) * np.power(old_stats.std, 2) + (new_stats.n - 1) * np.power(new_stats.std, 2) + old_stats.n * new_stats.n / (old_stats.n + new_stats.n) * (np.power(old_stats.mean, 2) + np.power(new_stats.mean, 2) - 2 * old_stats.mean * new_stats.mean)) / (old_stats.n + new_stats.n - 1))\n",
        "\n",
        "            # N1 - 1 * SD1^2\n",
        "            t1 = np.ma.add(old_stats.n, -1)\n",
        "            t2 = np.ma.power(old_stats.std, 2)\n",
        "            tr = np.ma.multiply(t1, t2)\n",
        "\n",
        "            # N2 - 1 * SD2^2\n",
        "            t1 = np.ma.add(new_stats.n, -1)\n",
        "            t2 = np.ma.power(new_stats.std, 2)\n",
        "            ts = np.ma.add(t1, t2)\n",
        "\n",
        "            # (N1*N2)/(N1+N2)\n",
        "            t1 = np.ma.multiply(old_stats.n, new_stats.n)\n",
        "            t2 = np.ma.add(old_stats.n, new_stats.n)\n",
        "            tt = np.ma.divide(t1, t2)\n",
        "\n",
        "            # (M1^2 + M2^2)\n",
        "            t1 = np.ma.power(old_stats.mean, 2)\n",
        "            t2 = np.ma.power(new_stats.mean, 2)\n",
        "            tu = np.ma.add(t1, t2)\n",
        "\n",
        "            # 2*M1*M2\n",
        "            tv = np.ma.multiply(np.ma.multiply(old_stats.mean, new_stats.mean), 2)\n",
        "\n",
        "            # N1 + N2 -1\n",
        "            tx = np.ma.add(np.ma.add(old_stats.n, new_stats.n), -1)\n",
        "\n",
        "            xr = np.ma.add(tr, ts)\n",
        "            xs = np.ma.subtract(tu, tv)\n",
        "            xt = np.ma.multiply(tt, xs)\n",
        "            xu = np.ma.add(xr, xt)\n",
        "\n",
        "            z = np.ma.divide(xu, tx)\n",
        "\n",
        "            tsd = np.ma.sqrt(z)\n",
        "\n",
        "            new_stats.n = tn\n",
        "            new_stats.mean = tmean\n",
        "            new_stats.std = tsd\n",
        "            self.statistics[zone] = new_stats\n",
        "\n",
        "    def update_multiple(self, zone):\n",
        "        \"\"\"\n",
        "        Iterates over the statistics collection, merging mean, std, and n\n",
        "        :param zone:\n",
        "        :param new_stats:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        # Update the last of the stat objects\n",
        "        self.statistics[zone][-1].calc_stats()\n",
        "\n",
        "        def collect(data, tn, tx, txx):\n",
        "            n = data.n\n",
        "            mean = data.mean\n",
        "            sd = data.std\n",
        "            x = n * mean\n",
        "            xx = sd**2 * (n - 1) + x**2 / n\n",
        "            tn = tn + n\n",
        "            tx = tx + x\n",
        "            txx = txx + xx\n",
        "\n",
        "            return tn, tx, txx\n",
        "\n",
        "        tn, tx, txx = 0, 0, 0\n",
        "        for data in self.statistics[zone]:\n",
        "            tn, tx, txx = collect(data, tn, tx, txx)\n",
        "\n",
        "        tmean = tx / tn\n",
        "        tsd = np.ma.sqrt(\n",
        "            np.ma.divide(\n",
        "                np.ma.subtract(txx, np.ma.divide(np.ma.power(tx, 2), tn)),\n",
        "                np.ma.add(tn, -1),\n",
        "            )\n",
        "        )\n",
        "\n",
        "        old_stats = self.statistics[zone][0]\n",
        "        old_stats.mean = tmean\n",
        "        old_stats.std = tsd\n",
        "        old_stats.n = tn\n",
        "\n",
        "        self.merged_stats[zone] = old_stats\n",
        "\n",
        "        return\n",
        "\n",
        "    def merge(self):\n",
        "        self.merged_stats = dict()\n",
        "\n",
        "        def chunk_gen(lst, n):\n",
        "            for i in range(0, len(lst), n):\n",
        "                yield lst[i : i + n]\n",
        "\n",
        "        def merge_chunk(indexes):\n",
        "            key_list = list(self.statistics.keys())\n",
        "            for i in indexes:\n",
        "                zone = key_list[i]\n",
        "                self.update_multiple(zone)\n",
        "\n",
        "        # with concurrent.futures.ProcessPoolExecutor() as executor:\n",
        "        #     futures = set()\n",
        "        #     n = 100\n",
        "        #     i_list = list(range(len(self.statistics)))\n",
        "        #     chunks = chunk_gen(i_list, n)\n",
        "        #     [futures.add(executor.submit(merge_chunk, c)) for c in chunks]\n",
        "        #     _, __ = concurrent.futures.wait(futures, return_when=concurrent.futures.ALL_COMPLETED)\n",
        "\n",
        "        [self.update_multiple(z) for z in self.statistics]\n",
        "        self.statistics = self.merged_stats\n",
        "        del self.merged_stats\n",
        "        return\n",
        "\n",
        "    def write(self, path=\"./output/zone_stats.csv\"):\n",
        "\n",
        "        with open(path, \"w\") as f:\n",
        "            header = \"zone, year, mean, std, n\\n\"\n",
        "            f.write(header)\n",
        "\n",
        "            for z in self.statistics:\n",
        "                data = self.statistics[z]\n",
        "                for i in range(len(data.mean)):\n",
        "                    line = \"{0}, {1}, {2}, {3}, {4}\\n\".format(\n",
        "                        z, i, data.mean[i], data.std[i], data.n[i]\n",
        "                    )\n",
        "                    f.write(line)\n",
        "\n",
        "\n",
        "class ZonalStatistics(object):\n",
        "    def __init__(self):\n",
        "\n",
        "        return\n",
        "\n",
        "    def data_collector(self, *args, **kwargs):\n",
        "\n",
        "        stats = dict()\n",
        "\n",
        "        I = args[0][\"zone_data\"].shape[0]\n",
        "        J = args[0][\"zone_data\"].shape[1]\n",
        "        K = args[0][\"zone_data\"].shape[2]\n",
        "\n",
        "        for i in range(I):\n",
        "            for j in range(J):\n",
        "                for k in range(K):\n",
        "                    zone = args[0][\"zone_data\"][i, j, k]\n",
        "                    data = args[0][\"val_data\"][:, j, k]\n",
        "\n",
        "                    if zone > 0:\n",
        "                        if zone not in stats:\n",
        "                            stats[zone] = Stat(zone)\n",
        "                        stats[zone].add_data(data)\n",
        "\n",
        "        return stats\n",
        "\n",
        "    def t_test(self, *args, **kwargs):\n",
        "\n",
        "        I = args[0][\"zone_data\"].shape[0]\n",
        "        J = args[0][\"zone_data\"].shape[1]\n",
        "        K = args[0][\"zone_data\"].shape[2]\n",
        "\n",
        "        output = np.empty((4, J, K), dtype=\"float32\")\n",
        "        output.fill(nodata)\n",
        "\n",
        "        for i in range(I):\n",
        "            for j in range(J):\n",
        "                for k in range(K):\n",
        "                    zone = args[0][\"zone_data\"][i, j, k]\n",
        "                    data = args[0][\"val_data\"][:, j, k]\n",
        "\n",
        "                    if zone > 0:\n",
        "                        stat = args[0][\"statistics\"].statistics[zone]\n",
        "                        try:\n",
        "                            vals = self._t_test_strict_r_logic(stat, data)\n",
        "                            if vals is not None:\n",
        "                                output[:, j, k] = vals\n",
        "                        except Exception as ex:\n",
        "                            # print(ex)\n",
        "                            pass\n",
        "\n",
        "        # This should be done all at once at the end, as it uses relative magnitudes of p to correct\n",
        "        # output = np.ma.masked_where(output == nodata, output)\n",
        "\n",
        "        # _, adj_p = multipletests(output[1, :, :], method='fdr_bh')\n",
        "        # output[1, :, :] = adj_p\n",
        "        # _, adj_p = multipletests(output[3, :, :], method='fdr_bh')\n",
        "        # output[3, :, :] = adj_p\n",
        "        # # adj_p = fdrcorrection(adj_p)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def _t_test_orig_logic(self, stat, data):\n",
        "        # Mask out NoData pixels\n",
        "        data = np.ma.masked_where(data < 0, data)\n",
        "        # Get the individual stats\n",
        "        i_mean = np.ma.mean(data)  # This is a single value (mean over time)\n",
        "\n",
        "        # Bail early if there's not data\n",
        "        if np.ma.is_masked(i_mean):\n",
        "            return None\n",
        "\n",
        "        i_std = np.ma.std(data)\n",
        "        i_n = np.ma.count(data)\n",
        "        i_se = i_std / np.ma.sqrt(i_n)\n",
        "\n",
        "        # Adjust population stats to remove point\n",
        "        #### NEED TO ADJUST FOR WEIGHTED TEMPORAL MEAN\n",
        "        # The -1 operations are because we removed a datapoint... maybe make this a variable\n",
        "        p_n_adj = stat.n - 1\n",
        "        p_mean_list = ((stat.mean * p_n_adj) - data) / p_n_adj  # This value is a list\n",
        "\n",
        "        p_n_sum = np.ma.sum(p_n_adj)\n",
        "        p_weights = np.ma.divide(stat.n, p_n_sum)\n",
        "        p_weighted_mean = p_mean_list * p_weights\n",
        "\n",
        "        p_mean = np.ma.sum(p_weighted_mean)\n",
        "        p_std = np.ma.std(p_weighted_mean)\n",
        "        p_n = np.ma.sum(p_n_adj)\n",
        "        p_se = p_std / np.ma.sqrt(p_n)\n",
        "\n",
        "        # Get the mean difference\n",
        "        mean_diff = i_mean - p_mean\n",
        "        # Standard error difference\n",
        "        se = np.ma.sqrt(i_se**2, p_se**2)\n",
        "\n",
        "        # t test\n",
        "        t = mean_diff / se\n",
        "\n",
        "        # degrees of freedom\n",
        "        df = i_n + p_n - 2\n",
        "\n",
        "        # p value\n",
        "        # p = stats.t.cdf(np.abs(t), df=df) * 2\n",
        "\n",
        "        # Adjust p for FDR\n",
        "\n",
        "        years = list(range(1, len(p_mean_list)))\n",
        "        pop_trend_model = OLS(p_mean_list, years)\n",
        "        pop_trend_result = pop_trend_model.fit()\n",
        "\n",
        "        ind_trend_model = GLSAR(data, years)\n",
        "        ind_trend_result = ind_trend_model.fit()\n",
        "\n",
        "        i = 1\n",
        "\n",
        "    def _t_test_strict_r_logic(self, stat, data):\n",
        "        # Mask out NoData pixels\n",
        "        data = np.ma.masked_where(data < 0, data)\n",
        "        # Get the individual stats\n",
        "        i_mean = np.ma.mean(data)  # This is a single value (mean over time)\n",
        "\n",
        "        # Bail early if there's not data\n",
        "        if np.ma.is_masked(i_mean):\n",
        "            return None\n",
        "\n",
        "        # Nan the missing values (statsmodels doesn't seem to acknowledge masked arrays)\n",
        "        nan_data = data.astype(float).filled(np.nan)\n",
        "        # i_mean_model = GLSAR(nan_data, missing='drop')\n",
        "        # i_mean_result = i_mean_model.fit()\n",
        "        # i_se = i_mean_result.bse\n",
        "\n",
        "        # Adjust population stats to remove point\n",
        "        # The -1 operations are because we removed a data point\n",
        "        p_n_adj = stat.n - 1\n",
        "        p_mean_list = ((stat.mean * p_n_adj) - data) / p_n_adj  # This value is a list\n",
        "        p_mean_list = np.ma.masked_where(p_mean_list < 0, p_mean_list)\n",
        "        # p_mean = np.ma.mean(p_mean_list)  # This is a single value\n",
        "\n",
        "        # Get the mean difference\n",
        "        # mean_diff = i_mean - p_mean\n",
        "\n",
        "        # t test\n",
        "        # t = mean_diff / i_se\n",
        "\n",
        "        # Skip doing the calculations manually, just do a basic t test\n",
        "        t, p = st.ttest_rel(nan_data, p_mean_list, nan_policy=\"omit\")\n",
        "\n",
        "        if np.ma.is_masked(t):\n",
        "            return None\n",
        "\n",
        "        # degrees of freedom\n",
        "        # df = i_mean_result.nobs - len(i_mean_result.params)\n",
        "\n",
        "        # p value\n",
        "        # p = st.t.cdf(np.abs(t), df=df) * 2\n",
        "\n",
        "        # Make years list for regressions\n",
        "        years = np.array(list(range(len(p_mean_list))))\n",
        "\n",
        "        # Nan years where there's missing data\n",
        "        mask_years = np.ma.array(years.astype(float), mask=p_mean_list.mask)\n",
        "        nan_years = mask_years.filled(np.nan)\n",
        "        pop_trend_model = GLSAR(p_mean_list, nan_years, missing=\"drop\")\n",
        "        pop_trend_result = pop_trend_model.fit()\n",
        "\n",
        "        mask_years = np.ma.array(years.astype(float), mask=data.mask)\n",
        "        nan_years = mask_years.filled(np.nan)\n",
        "        ind_trend_model = GLSAR(data, nan_years, missing=\"drop\")\n",
        "        ind_trend_result = ind_trend_model.fit()\n",
        "\n",
        "        pop_slope = pop_trend_result.params[0]\n",
        "        ind_slope = ind_trend_result.params[0]\n",
        "\n",
        "        slope_diff = ind_slope - pop_slope\n",
        "        slope_se = ind_trend_result.bse\n",
        "        # slope_pop_se = pop_trend_result.bse\n",
        "        slope_t = slope_diff / slope_se\n",
        "\n",
        "        # slope_n = len(ind_trend_model.endog)\n",
        "        # slope_pop_n = len(pop_trend_model.endog)\n",
        "\n",
        "        df = ind_trend_result.nobs - len(ind_trend_result.params)\n",
        "\n",
        "        slope_p = st.t.sf(np.abs(slope_t), df=df) * 2\n",
        "\n",
        "        # slope_t, slope_p = st.ttest_ind_from_stats(ind_slope, slope_se, slope_n, pop_slope, slope_pop_se, slope_pop_n, equal_var=False)\n",
        "\n",
        "        # vals = np.array([t[0], p[0], slope_t[0], slope_p[0]])\n",
        "        vals = np.array([t, p, slope_t[0], slope_p[0]])\n",
        "\n",
        "        return vals\n",
        "\n",
        "class Degradation(object):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        # self.zone_raster_path = kwargs['zone_raster']\n",
        "\n",
        "        # TODO check for banded raster vs list of rasters\n",
        "        # self.data_raster_path = kwargs['data_raster']\n",
        "        return\n",
        "\n",
        "    def degradation(self, data):\n",
        "\n",
        "        I = data.shape[0]\n",
        "        J = data.shape[1]\n",
        "        K = data.shape[2]\n",
        "        output = np.empty((I, J, K))\n",
        "\n",
        "        for i in range(I):\n",
        "            for j in range(J):\n",
        "                for k in range(K):\n",
        "                    val = data[i, j, k]\n",
        "                    output[i, j, k] = val\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "XYEyGA_xzElN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f6f10ae-157c-473a-c70a-22e596ea2a3b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inputs"
      ],
      "metadata": {
        "id": "0R8L-0Vp3BoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zone_name = \"BpsZonRobGb_wgs84_nc\"\n",
        "gcs_degradation_path = \"gs://fuelcast-data/degradation/\"\n",
        "gcs_rpms_path = \"gs://fuelcast-data/rpms/\"\n",
        "\n",
        "zone_raster_path = f\"{gcs_degradation_path}{zone_name}/{zone_name}.tif\"\n",
        "data_raster_path = f\"./data/{zone_name}/rpms_stack.tif\"\n",
        "dummy_path = \"./test.tif\""
      ],
      "metadata": {
        "id": "Ohw4agRL1Ugb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_path = [\n",
        "    f\"./output/{zone_name}_mean_t.tif\",\n",
        "    f\"./output/{zone_name}_mean_p_adj.tif\",\n",
        "    f\"./output/{zone_name}_slope_t.tif\",\n",
        "    f\"./output/{zone_name}_slope_p_adj.tif\",\n",
        "]\n",
        "\n",
        "if not os.path.exists(\"./output/\"):\n",
        "    os.makedirs(\"./output/\")\n",
        "\n",
        "# stats_pickle_path = f\"./output/{zone_name}_zs.pkl\"\n",
        "\n",
        "\n",
        "BLOCKSIZE = 8196\n",
        "\n",
        "nodata = -3.4e38\n",
        "\n",
        "\n",
        "def main_process(task, zone_file, data_file, out_file, queue_size=1):\n",
        "    \"\"\"\n",
        "    This function constantly tries to write to the destination raster, which makes it suitable for tasks that are\n",
        "    done in one pass of the data. For things requiring multiple passes, use a function that doesn't write until\n",
        "    the data processing is done.\n",
        "    :param task:\n",
        "    :param zone_file:\n",
        "    :param data_file:\n",
        "    :param out_file:\n",
        "    :param queue_size:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    deg = Degradation()\n",
        "\n",
        "    func = deg.degradation\n",
        "\n",
        "    # Start rasterio and set environment variables as needed\n",
        "    with rasterio.Env():\n",
        "        with rasterio.open(zone_file) as zone_src:\n",
        "            # Copy the zone_src dataset parameters for the output dataset, and set up tiles\n",
        "            profile = zone_src.profile\n",
        "            profile.update(blockxsize=BLOCKSIZE, blockysize=BLOCKSIZE, tiled=True)\n",
        "\n",
        "            with rasterio.open(out_file, \"w\", **profile) as dst:\n",
        "                with rasterio.open(data_file) as data_src:\n",
        "\n",
        "                    zone_windows = [window for ij, window in dst.block_windows()]\n",
        "                    # This will track remaining zone_windows. Might\n",
        "                    window_count = len(zone_windows)\n",
        "                    print(\"Window Count:\", window_count)\n",
        "\n",
        "                    # BoundedProcessPoolExecutor expands concurrent.futures.ProcessPoolExecutor to include a semaphore\n",
        "                    # that blocks process creation when max_workers are active. This keeps memory footprint low.\n",
        "                    with BoundedProcessPoolExecutor(max_workers=queue_size) as executor:\n",
        "\n",
        "                        # Create a streaming iterator for zone_windows\n",
        "                        def stream():\n",
        "                            yield from iter(zone_windows)\n",
        "\n",
        "                        # Just a compact function for reading the zone_src dataset\n",
        "                        def read_zone_ds(window):\n",
        "                            return zone_src.read(window=window)\n",
        "\n",
        "                        def read_data(z_window):\n",
        "                            bounds = zone_src.window_bounds(z_window)\n",
        "                            data_window = data_src.window(*bounds)\n",
        "                            return data_src.read(window=data_window)\n",
        "\n",
        "                        streamer = stream()\n",
        "\n",
        "                        # This gets redefined every time the streamer iterates, and the object is a set of not\n",
        "                        # done futures\n",
        "                        futures = set()\n",
        "\n",
        "                        # This is our own collection that remembers what window the future object used. It's not reset\n",
        "                        # every iteration so del finished futures to keep memory low\n",
        "                        futures_and_windows = dict()\n",
        "\n",
        "                        # Note that using two collections effectively double active memory footprint... maybe there's\n",
        "                        # a way around this. But, a set is what the concurrent.futures.wait returns, and that's how you\n",
        "                        # write finished data and move on.\n",
        "\n",
        "                        # Process each window\n",
        "                        for w in streamer:\n",
        "                            # Multiple zone_windows can finish simultaneously (see below), so attempt to fill the\n",
        "                            # semaphore every time\n",
        "                            for i in range(queue_size - len(futures)):\n",
        "                                try:\n",
        "                                    window = next(streamer)\n",
        "                                    ex = executor.submit(\n",
        "                                        func, read_zone_ds(window), read_data(window)\n",
        "                                    )\n",
        "                                    futures_and_windows[ex] = window\n",
        "                                    futures.add(ex)\n",
        "                                except StopIteration:\n",
        "                                    pass\n",
        "\n",
        "                            # Add the window from the original streamer generator\n",
        "                            ex = executor.submit(\n",
        "                                func, read_zone_ds(w), read_data(window)\n",
        "                            )\n",
        "                            futures_and_windows[ex] = w\n",
        "                            futures.add(ex)\n",
        "\n",
        "                            # When at least one future finishes, get the completed data and do what you want to\n",
        "                            done, futures = concurrent.futures.wait(\n",
        "                                futures, return_when=concurrent.futures.FIRST_COMPLETED\n",
        "                            )\n",
        "\n",
        "                            for future in done:\n",
        "                                data = future.result()\n",
        "                                window = futures_and_windows[future]\n",
        "\n",
        "                                window_count -= 1\n",
        "                                print(\n",
        "                                    f\"Remaining: {window_count} || {window} || Size of futures: {len(futures)}\"\n",
        "                                )\n",
        "\n",
        "                                dst.write(data, window=window)\n",
        "                                del futures_and_windows[future]\n",
        "\n",
        "                        # Finish remaining tasks after all zone_windows have been assigned\n",
        "                        done, futures = concurrent.futures.wait(\n",
        "                            futures, return_when=concurrent.futures.ALL_COMPLETED\n",
        "                        )\n",
        "\n",
        "                        for future in done:\n",
        "                            data = future.result()\n",
        "                            window = futures_and_windows[future]\n",
        "                            print(f\"Writing data: window={window}\")\n",
        "                            # with write_lock:\n",
        "                            dst.write(data, window=window)\n",
        "\n",
        "                            del futures_and_windows[ex]\n",
        "    return\n",
        "\n",
        "\n",
        "def main_statistics(\n",
        "    task, zone_file, data_file, out_files, queue_size=10, *args, **kwargs\n",
        "):\n",
        "    zs = ZonalStatistics()\n",
        "\n",
        "    if task == \"collect\":\n",
        "        accumulator = StatAccumulator()\n",
        "        func = zs.data_collector\n",
        "    elif task == \"degradation\":\n",
        "        if \"acc\" in kwargs:\n",
        "            accumulator = kwargs[\"acc\"]\n",
        "        else:\n",
        "            raise ValueError()\n",
        "        func = zs.t_test\n",
        "\n",
        "    # Start rasterio and set environment variables as needed\n",
        "    with rasterio.Env():\n",
        "        with rasterio.open(zone_file) as zone_src:\n",
        "            # Copy the zone_src dataset parameters for the output dataset, and set up tiles\n",
        "            profile = zone_src.profile\n",
        "            profile.update(\n",
        "                blockxsize=BLOCKSIZE,\n",
        "                blockysize=BLOCKSIZE,\n",
        "                tiled=True,\n",
        "                dtype=\"float32\",\n",
        "                compress=\"DEFLATE\",\n",
        "                nodata=nodata,\n",
        "            )\n",
        "\n",
        "            if task == \"degradation\":\n",
        "                mean_t_raster = rasterio.open(out_files[0], \"w\", **profile)\n",
        "                mean_p_raster = rasterio.open(out_files[1], \"w\", **profile)\n",
        "                slope_t_raster = rasterio.open(out_files[2], \"w\", **profile)\n",
        "                slope_p_raster = rasterio.open(out_files[3], \"w\", **profile)\n",
        "\n",
        "            dummy = rasterio.open(dummy_path, \"w\", **profile)\n",
        "\n",
        "            with rasterio.open(data_file) as data_src:\n",
        "\n",
        "                zone_windows = [window for ij, window in dummy.block_windows()]\n",
        "                # This will track remaining zone_windows. Might\n",
        "                window_count = len(zone_windows)\n",
        "                print(\"Window Count:\", window_count)\n",
        "\n",
        "                # BoundedProcessPoolExecutor expands concurrent.futures.ProcessPoolExecutor to include a semaphore\n",
        "                # that blocks process creation when max_workers are active. This keeps memory footprint low.\n",
        "                with BoundedProcessPoolExecutor(max_workers=queue_size) as executor:\n",
        "\n",
        "                    # Create a streaming iterator for zone_windows\n",
        "                    def stream():\n",
        "                        yield from iter(zone_windows)\n",
        "\n",
        "                    # Just a compact function for reading the zone_src dataset\n",
        "                    def read_zone_ds(z_window):\n",
        "                        return zone_src.read(window=z_window)\n",
        "\n",
        "                    # For handling different extents (bot not projections!), pass the zone window, convert to\n",
        "                    # lat/long, and get the appropriate window for the data\n",
        "                    def read_data(z_window):\n",
        "                        bounds = zone_src.window_bounds(z_window)\n",
        "                        d_window = data_src.window(*bounds)\n",
        "                        return data_src.read(window=d_window), d_window\n",
        "\n",
        "                    # Set up a window generator\n",
        "                    streamer = stream()\n",
        "\n",
        "                    # This gets redefined every time the streamer iterates, and the object is a set of not\n",
        "                    # done futures\n",
        "                    futures = set()\n",
        "\n",
        "                    # This is our own collection that remembers what window the future object used. It's not reset\n",
        "                    # every iteration so del finished futures to keep memory low\n",
        "                    futures_and_windows = dict()\n",
        "\n",
        "                    # Process each window\n",
        "                    for w in streamer:\n",
        "                        # Multiple zone_windows can finish simultaneously (see below), so attempt to fill the\n",
        "                        # semaphore every time\n",
        "                        for i in range(queue_size - len(futures)):\n",
        "                            try:\n",
        "                                stream_window = next(streamer)\n",
        "                                zone = read_zone_ds(stream_window)\n",
        "                                data, data_window = read_data(stream_window)\n",
        "\n",
        "                                func_args = {\"zone_data\": zone, \"val_data\": data}\n",
        "\n",
        "                                if task == \"degradation\":\n",
        "                                    func_args[\"statistics\"] = accumulator\n",
        "\n",
        "                                ex = executor.submit(func, func_args)\n",
        "                                futures.add(ex)\n",
        "                                futures_and_windows[ex] = stream_window\n",
        "                            except StopIteration:\n",
        "                                pass\n",
        "\n",
        "                        # Add the window from the original streamer generator\n",
        "                        data, data_window = read_data(w)\n",
        "                        zone = read_zone_ds(w)\n",
        "\n",
        "                        func_args = {\"zone_data\": zone, \"val_data\": data}\n",
        "\n",
        "                        if task == \"degradation\":\n",
        "                            func_args[\"statistics\"] = accumulator\n",
        "\n",
        "                        ex = executor.submit(func, func_args)\n",
        "                        futures.add(ex)\n",
        "                        futures_and_windows[ex] = w\n",
        "\n",
        "                        # When at least one future finishes, get the completed data and do what you want to\n",
        "                        done, futures = concurrent.futures.wait(\n",
        "                            futures, return_when=concurrent.futures.FIRST_COMPLETED\n",
        "                        )\n",
        "\n",
        "                        for future in done:\n",
        "                            data = future.result()\n",
        "                            window = futures_and_windows[future]\n",
        "\n",
        "                            if task == \"collect\":\n",
        "                                [accumulator.update(zone, data[zone]) for zone in data]\n",
        "                            else:\n",
        "                                data = [\n",
        "                                    data[i, :, :].reshape(\n",
        "                                        1, data.shape[1], data.shape[2]\n",
        "                                    )\n",
        "                                    for i in range(4)\n",
        "                                ]\n",
        "                                mean_t_raster.write(data[0], window=window)\n",
        "                                mean_p_raster.write(data[1], window=window)\n",
        "                                slope_t_raster.write(data[2], window=window)\n",
        "                                slope_p_raster.write(data[3], window=window)\n",
        "\n",
        "                            window_count -= 1\n",
        "                            print(\n",
        "                                f\"Remaining: {window_count} || {window} || Size of futures: {len(futures)}\"\n",
        "                            )\n",
        "\n",
        "                            del futures_and_windows[future]\n",
        "\n",
        "                    # Finish remaining tasks after all zone_windows have been assigned\n",
        "                    done, futures = concurrent.futures.wait(\n",
        "                        futures, return_when=concurrent.futures.ALL_COMPLETED\n",
        "                    )\n",
        "\n",
        "                    for future in done:\n",
        "                        data = future.result()\n",
        "                        window = futures_and_windows[future]\n",
        "\n",
        "                        if task == \"collect\":\n",
        "                            [accumulator.update(zone, data[zone]) for zone in data]\n",
        "                        else:\n",
        "                            data = [\n",
        "                                data[i, :, :].reshape(1, data.shape[1], data.shape[2])\n",
        "                                for i in range(4)\n",
        "                            ]\n",
        "                            mean_t_raster.write(data[0], window=window)\n",
        "                            mean_p_raster.write(data[1], window=window)\n",
        "                            slope_t_raster.write(data[2], window=window)\n",
        "                            slope_p_raster.write(data[3], window=window)\n",
        "\n",
        "                        window_count -= 1\n",
        "                        print(\n",
        "                            f\"Remaining: {window_count} || {window} || Size of futures: {len(futures)}\"\n",
        "                        )\n",
        "\n",
        "                        del futures_and_windows[future]\n",
        "\n",
        "                    if task == \"collect\":\n",
        "                        # Merge the collected statistic objects\n",
        "                        print(\"Merging collected statistics\")\n",
        "                        accumulator.merge()\n",
        "\n",
        "                        print(\"Writing statistics to file\")\n",
        "                        accumulator.write()\n",
        "\n",
        "                        # with open(stats_pickle_path, \"wb\") as f:\n",
        "                        #     pickle.dump(accumulator, f)\n",
        "\n",
        "                    else:\n",
        "                        print(\"uploading\")\n",
        "                        # gch.upload_blob(\"fuelcast-data\",mean_t_raster,\"degradation/BpsZonRobGb_wgs84_nc/mean_t.tif\")\n",
        "                        # gch.upload_blob(\"fuelcast-data\",mean_p_raster,\"degradation/BpsZonRobGb_wgs84_nc/mean_p_adj.tif\")\n",
        "                        # gch.upload_blob(\"fuelcast-data\",slope_t_raster,\"degradation/BpsZonRobGb_wgs84_nc/slope_t.tif\")\n",
        "                        # gch.upload_blob(\"fuelcast-data\",slope_p_raster,\"degradation/BpsZonRobGb_wgs84_nc/slope_p_adj.tif\")\n",
        "                        mean_t_raster.close()\n",
        "                        mean_p_raster.close()\n",
        "                        slope_t_raster.close()\n",
        "                        slope_p_raster.close()\n",
        "                        dummy.close()\n",
        "\n",
        "                        os.remove(dummy_path)\n",
        "\n",
        "        return accumulator\n",
        "\n",
        "# async def raster_stacker(in_ds, out_ds, bounds):\n",
        "def raster_stacker(id, in_ds, out_ds, bounds):\n",
        "    with rasterio.open(in_ds, chunks=(1, 1024, 1024), lock=False) as src_ds:\n",
        "        win = src_ds.window(\n",
        "            bottom=bounds.bottom,\n",
        "            right=bounds.right,\n",
        "            top=bounds.top,\n",
        "            left=bounds.left,\n",
        "        )\n",
        "        print(f\"in: {in_ds} || {win}\")\n",
        "        out_ds.write_band(id, src_ds.read(1, window=win))\n",
        "\n",
        "\n",
        "async def main_run():\n",
        "    with rasterio.Env(GDAL_NUM_THREADS=\"ALL_CPUS\", verbose=2, GOOGLE_APPLICATION_CREDENTIALS=os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\", path_to_credentials)):\n",
        "        zone_ds = rasterio.open(zone_raster_path, chunks=(1024, 1024))\n",
        "        bounds = zone_ds.bounds\n",
        "        profile = zone_ds.profile\n",
        "        profile.update(\n",
        "            blockxsize=1024,\n",
        "            blockysize=1024,\n",
        "            tiled=True,\n",
        "            compress=\"DEFLATE\",\n",
        "            predictor=2,\n",
        "            BIGTIFF=\"Yes\",\n",
        "        )\n",
        "\n",
        "        od = f\"./data/{zone_name}\"\n",
        "        if not os.path.exists(od):\n",
        "            os.makedirs(od)\n",
        "\n",
        "        files = list()\n",
        "        for y in range(1985, 2022):\n",
        "            if y == 2012:\n",
        "                continue\n",
        "            # f = f\"./data/{zone_name}/rpms_{y}_mean.tif\"\n",
        "            f = f\"gs://fuelcast-data/rpms/{y}/rpms_{y}.tif\"\n",
        "            files.append(f)\n",
        "\n",
        "        meta = zone_ds.meta\n",
        "        meta.update(count=len(files))\n",
        "        profile.update(count=len(files))\n",
        "\n",
        "        print(\"Stacking raster\")\n",
        "\n",
        "        stack_path = f\"./data/{zone_name}/rpms_stack.tif\"\n",
        "\n",
        "        if os.path.exists(stack_path):\n",
        "            print(f\"Stacked raster {stack_path} already exists.\")\n",
        "        else:\n",
        "            with rasterio.open(stack_path, \"w\", **profile) as dst:\n",
        "                print(f\"out: {dst} || {dst.bounds}\")\n",
        "\n",
        "                for id, layer in enumerate(files, start=1):\n",
        "                    print(f\"in: {layer}\")\n",
        "                    raster_stacker(id, layer, dst, bounds)\n",
        "\n",
        "\n",
        "\n",
        "        print(\"Calculating zonal statistics\")\n",
        "\n",
        "        # if os.path.exists(stats_pickle_path):\n",
        "        #     print(\"Found existing statistics file. Loading.\")\n",
        "        # else:\n",
        "        acc = main_statistics(\n",
        "            \"collect\", zone_raster_path, data_raster_path, out_path, 60\n",
        "        )\n",
        "\n",
        "        # with open(stats_pickle_path, \"rb\") as f:\n",
        "        #     acc = pickle.load(f)\n",
        "\n",
        "        print(\"Running degradation\")\n",
        "        start = datetime.now()\n",
        "        main_statistics(\n",
        "            \"degradation\", zone_raster_path, data_raster_path, out_path, 60, acc=acc\n",
        "        )\n",
        "        stop = datetime.now()\n",
        "        print(\"Total runtime:\", (stop - start).seconds / 60, \"minutes\")\n",
        "\n",
        "        print(\"Finished\")\n",
        "\n",
        "await main_run()"
      ],
      "metadata": {
        "id": "6aPkpw5Z3G7h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cfe8924-8428-48aa-9614-1db1a828156b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:rasterio._env:CPLE_NotSupported in driver GTiff does not support open option CHUNKS\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking raster\n",
            "out: <open DatasetWriter name='./data/BpsZonRobGb_wgs84_nc/rpms_stack.tif' mode='w'> || BoundingBox(left=-123.00035763899075, bottom=32.999762673956695, right=-99.99953377828074, top=50.000289594390246)\n",
            "in: gs://fuelcast-data/rpms/1985/rpms_1985.tif\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:rasterio._env:CPLE_NotSupported in driver GTiff does not support open option CHUNKS\n",
            "WARNING:rasterio._env:CPLE_NotSupported in driver GTiff does not support open option LOCK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in: gs://fuelcast-data/rpms/1985/rpms_1985.tif || Window(col_off=6859.760674567195, row_off=-2284.559238791233, width=85348.00000000047, height=63083.00000000006)\n"
          ]
        }
      ]
    }
  ]
}