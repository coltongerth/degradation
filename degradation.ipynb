{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/AUF0kNmtleTpPe/GPmQj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coltongerth/degredation/blob/main/degradation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dT7SoAoSugrt"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install affine==2.3.1\n",
        "!pip install attrs==22.2.0\n",
        "!pip install black==22.12.0\n",
        "!pip install bounded-pool-executor==0.0.3\n",
        "!pip install certifi==2022.12.7\n",
        "!pip install click==8.1.3\n",
        "!pip install click-plugins==1.1.1\n",
        "!pip install cligj==0.7.2\n",
        "!pip install mypy-extensions==0.4.3\n",
        "!pip install numpy==1.24.1\n",
        "!pip install packaging==23.0\n",
        "!pip install pandas==1.5.2\n",
        "!pip install pathspec==0.10.3\n",
        "!pip install patsy==0.5.3\n",
        "!pip install platformdirs==2.6.2\n",
        "!pip install pyparsing==3.0.9\n",
        "!pip install python-dateutil==2.8.2\n",
        "!pip install pytz==2022.7\n",
        "!pip install rasterio==1.3.4\n",
        "!pip install scipy==1.10.0\n",
        "!pip install six==1.16.0\n",
        "!pip install snuggs==1.4.7\n",
        "!pip install statsmodels==0.13.5\n",
        "!pip install tomli==2.0.1\n",
        "!pip install git+https://github.com/lankston-consulting/lcutils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent.futures\n",
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "import rasterio\n",
        "import asyncio\n",
        "import warnings\n",
        "from statsmodels.regression.linear_model import OLS, GLSAR\n",
        "from scipy import stats as st\n",
        "from datetime import datetime\n",
        "from bounded_pool_executor import BoundedProcessPoolExecutor\n",
        "from dotenv import load_dotenv\n",
        "from lcutils import gcs, eet\n",
        "\n",
        "\n",
        "# gch = gcs.GcsTools(use_service_account={\"keyfile\": storage_keyfile})\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "nodata = -3.4e38\n",
        "\n",
        "\n",
        "class Stat(object):\n",
        "    def __init__(self, zone):\n",
        "        self.zone = zone\n",
        "        self.mean = 0\n",
        "        self.std = 0\n",
        "        self.n = 0\n",
        "        self.data = list()\n",
        "\n",
        "    def add_data(self, data):\n",
        "        self.data.append(data)\n",
        "\n",
        "    def calc_stats(self):\n",
        "        if self.data:\n",
        "            self.data = np.array(self.data)\n",
        "            self.data = np.ma.masked_where(self.data < 0, self.data)\n",
        "            self.mean = np.ma.mean(self.data, axis=0)\n",
        "            self.std = np.ma.std(self.data, axis=0)\n",
        "            self.n = np.ma.count(self.data, axis=0)\n",
        "            self.n = np.ma.masked_where(\n",
        "                self.n <= 0, self.n\n",
        "            )  # n of 0 screws up combining groups later\n",
        "\n",
        "            # Clean up the object to reduce memory footprint\n",
        "            del self.data\n",
        "\n",
        "\n",
        "class StatAccumulator(object):\n",
        "    def __init__(self, update_size=500):\n",
        "        # .statistics will be a collection keyed by zone that references a list of Stat objects. As data is collected\n",
        "        # and the Stat object gets to a determined size, statistics will be calculated and the data will be deleted.\n",
        "        # New data will go in the accumulator as a new stat object. Rinse and repeat.\n",
        "        self.statistics = dict()\n",
        "        self._update_size = update_size\n",
        "        self.merged_stats = dict()\n",
        "\n",
        "    def update(self, zone, new_stats, force=False):\n",
        "        if zone not in self.statistics:\n",
        "            self.statistics[zone] = list()\n",
        "            self.statistics[zone].append(new_stats)\n",
        "        else:\n",
        "            stats_col = self.statistics[zone]\n",
        "            old_stats = stats_col[-1]  # Get the latest stat collection\n",
        "\n",
        "            # If the latest record has over x records, create a new object\n",
        "            if len(old_stats.data) > self._update_size or force:\n",
        "                old_stats.calc_stats()  # Clean up the memory footprint\n",
        "                self.statistics[zone].append(new_stats)\n",
        "            else:\n",
        "                [old_stats.data.append(d) for d in new_stats.data]\n",
        "        return\n",
        "\n",
        "    def update_cochrane(self, zone, new_stats):\n",
        "        if zone not in self.statistics:\n",
        "            self.statistics[zone] = new_stats\n",
        "        else:\n",
        "            old_stats = self.statistics[zone]\n",
        "\n",
        "            tn = old_stats.n + new_stats.n\n",
        "            tmean = (\n",
        "                np.ma.add(old_stats.n * old_stats.mean, new_stats.n * new_stats.mean)\n",
        "                / tn\n",
        "            )\n",
        "\n",
        "            # tsd = np.ma.sqrt(((old_stats.n-1) * np.power(old_stats.std, 2) + (new_stats.n - 1) * np.power(new_stats.std, 2) + old_stats.n * new_stats.n / (old_stats.n + new_stats.n) * (np.power(old_stats.mean, 2) + np.power(new_stats.mean, 2) - 2 * old_stats.mean * new_stats.mean)) / (old_stats.n + new_stats.n - 1))\n",
        "\n",
        "            # N1 - 1 * SD1^2\n",
        "            t1 = np.ma.add(old_stats.n, -1)\n",
        "            t2 = np.ma.power(old_stats.std, 2)\n",
        "            tr = np.ma.multiply(t1, t2)\n",
        "\n",
        "            # N2 - 1 * SD2^2\n",
        "            t1 = np.ma.add(new_stats.n, -1)\n",
        "            t2 = np.ma.power(new_stats.std, 2)\n",
        "            ts = np.ma.add(t1, t2)\n",
        "\n",
        "            # (N1*N2)/(N1+N2)\n",
        "            t1 = np.ma.multiply(old_stats.n, new_stats.n)\n",
        "            t2 = np.ma.add(old_stats.n, new_stats.n)\n",
        "            tt = np.ma.divide(t1, t2)\n",
        "\n",
        "            # (M1^2 + M2^2)\n",
        "            t1 = np.ma.power(old_stats.mean, 2)\n",
        "            t2 = np.ma.power(new_stats.mean, 2)\n",
        "            tu = np.ma.add(t1, t2)\n",
        "\n",
        "            # 2*M1*M2\n",
        "            tv = np.ma.multiply(np.ma.multiply(old_stats.mean, new_stats.mean), 2)\n",
        "\n",
        "            # N1 + N2 -1\n",
        "            tx = np.ma.add(np.ma.add(old_stats.n, new_stats.n), -1)\n",
        "\n",
        "            xr = np.ma.add(tr, ts)\n",
        "            xs = np.ma.subtract(tu, tv)\n",
        "            xt = np.ma.multiply(tt, xs)\n",
        "            xu = np.ma.add(xr, xt)\n",
        "\n",
        "            z = np.ma.divide(xu, tx)\n",
        "\n",
        "            tsd = np.ma.sqrt(z)\n",
        "\n",
        "            new_stats.n = tn\n",
        "            new_stats.mean = tmean\n",
        "            new_stats.std = tsd\n",
        "            self.statistics[zone] = new_stats\n",
        "\n",
        "    def update_multiple(self, zone):\n",
        "        \"\"\"\n",
        "        Iterates over the statistics collection, merging mean, std, and n\n",
        "        :param zone:\n",
        "        :param new_stats:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        # Update the last of the stat objects\n",
        "        self.statistics[zone][-1].calc_stats()\n",
        "\n",
        "        def collect(data, tn, tx, txx):\n",
        "            n = data.n\n",
        "            mean = data.mean\n",
        "            sd = data.std\n",
        "            x = n * mean\n",
        "            xx = sd**2 * (n - 1) + x**2 / n\n",
        "            tn = tn + n\n",
        "            tx = tx + x\n",
        "            txx = txx + xx\n",
        "\n",
        "            return tn, tx, txx\n",
        "\n",
        "        tn, tx, txx = 0, 0, 0\n",
        "        for data in self.statistics[zone]:\n",
        "            tn, tx, txx = collect(data, tn, tx, txx)\n",
        "\n",
        "        tmean = tx / tn\n",
        "        tsd = np.ma.sqrt(\n",
        "            np.ma.divide(\n",
        "                np.ma.subtract(txx, np.ma.divide(np.ma.power(tx, 2), tn)),\n",
        "                np.ma.add(tn, -1),\n",
        "            )\n",
        "        )\n",
        "\n",
        "        old_stats = self.statistics[zone][0]\n",
        "        old_stats.mean = tmean\n",
        "        old_stats.std = tsd\n",
        "        old_stats.n = tn\n",
        "\n",
        "        self.merged_stats[zone] = old_stats\n",
        "\n",
        "        return\n",
        "\n",
        "    def merge(self):\n",
        "        self.merged_stats = dict()\n",
        "\n",
        "        def chunk_gen(lst, n):\n",
        "            for i in range(0, len(lst), n):\n",
        "                yield lst[i : i + n]\n",
        "\n",
        "        def merge_chunk(indexes):\n",
        "            key_list = list(self.statistics.keys())\n",
        "            for i in indexes:\n",
        "                zone = key_list[i]\n",
        "                self.update_multiple(zone)\n",
        "\n",
        "        # with concurrent.futures.ProcessPoolExecutor() as executor:\n",
        "        #     futures = set()\n",
        "        #     n = 100\n",
        "        #     i_list = list(range(len(self.statistics)))\n",
        "        #     chunks = chunk_gen(i_list, n)\n",
        "        #     [futures.add(executor.submit(merge_chunk, c)) for c in chunks]\n",
        "        #     _, __ = concurrent.futures.wait(futures, return_when=concurrent.futures.ALL_COMPLETED)\n",
        "\n",
        "        [self.update_multiple(z) for z in self.statistics]\n",
        "        self.statistics = self.merged_stats\n",
        "        del self.merged_stats\n",
        "        return\n",
        "\n",
        "    def write(self, path=\"./output/zone_stats.csv\"):\n",
        "\n",
        "        with open(path, \"w\") as f:\n",
        "            header = \"zone, year, mean, std, n\\n\"\n",
        "            f.write(header)\n",
        "\n",
        "            for z in self.statistics:\n",
        "                data = self.statistics[z]\n",
        "                for i in range(len(data.mean)):\n",
        "                    line = \"{0}, {1}, {2}, {3}, {4}\\n\".format(\n",
        "                        z, i, data.mean[i], data.std[i], data.n[i]\n",
        "                    )\n",
        "                    f.write(line)\n",
        "\n",
        "\n",
        "class ZonalStatistics(object):\n",
        "    def __init__(self):\n",
        "\n",
        "        return\n",
        "\n",
        "    def data_collector(self, *args, **kwargs):\n",
        "\n",
        "        stats = dict()\n",
        "\n",
        "        I = args[0][\"zone_data\"].shape[0]\n",
        "        J = args[0][\"zone_data\"].shape[1]\n",
        "        K = args[0][\"zone_data\"].shape[2]\n",
        "\n",
        "        for i in range(I):\n",
        "            for j in range(J):\n",
        "                for k in range(K):\n",
        "                    zone = args[0][\"zone_data\"][i, j, k]\n",
        "                    data = args[0][\"val_data\"][:, j, k]\n",
        "\n",
        "                    if zone > 0:\n",
        "                        if zone not in stats:\n",
        "                            stats[zone] = Stat(zone)\n",
        "                        stats[zone].add_data(data)\n",
        "\n",
        "        return stats\n",
        "\n",
        "    def t_test(self, *args, **kwargs):\n",
        "\n",
        "        I = args[0][\"zone_data\"].shape[0]\n",
        "        J = args[0][\"zone_data\"].shape[1]\n",
        "        K = args[0][\"zone_data\"].shape[2]\n",
        "\n",
        "        output = np.empty((4, J, K), dtype=\"float32\")\n",
        "        output.fill(nodata)\n",
        "\n",
        "        for i in range(I):\n",
        "            for j in range(J):\n",
        "                for k in range(K):\n",
        "                    zone = args[0][\"zone_data\"][i, j, k]\n",
        "                    data = args[0][\"val_data\"][:, j, k]\n",
        "\n",
        "                    if zone > 0:\n",
        "                        stat = args[0][\"statistics\"].statistics[zone]\n",
        "                        try:\n",
        "                            vals = self._t_test_strict_r_logic(stat, data)\n",
        "                            if vals is not None:\n",
        "                                output[:, j, k] = vals\n",
        "                        except Exception as ex:\n",
        "                            # print(ex)\n",
        "                            pass\n",
        "\n",
        "        # This should be done all at once at the end, as it uses relative magnitudes of p to correct\n",
        "        # output = np.ma.masked_where(output == nodata, output)\n",
        "\n",
        "        # _, adj_p = multipletests(output[1, :, :], method='fdr_bh')\n",
        "        # output[1, :, :] = adj_p\n",
        "        # _, adj_p = multipletests(output[3, :, :], method='fdr_bh')\n",
        "        # output[3, :, :] = adj_p\n",
        "        # # adj_p = fdrcorrection(adj_p)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def _t_test_orig_logic(self, stat, data):\n",
        "        # Mask out NoData pixels\n",
        "        data = np.ma.masked_where(data < 0, data)\n",
        "        # Get the individual stats\n",
        "        i_mean = np.ma.mean(data)  # This is a single value (mean over time)\n",
        "\n",
        "        # Bail early if there's not data\n",
        "        if np.ma.is_masked(i_mean):\n",
        "            return None\n",
        "\n",
        "        i_std = np.ma.std(data)\n",
        "        i_n = np.ma.count(data)\n",
        "        i_se = i_std / np.ma.sqrt(i_n)\n",
        "\n",
        "        # Adjust population stats to remove point\n",
        "        #### NEED TO ADJUST FOR WEIGHTED TEMPORAL MEAN\n",
        "        # The -1 operations are because we removed a datapoint... maybe make this a variable\n",
        "        p_n_adj = stat.n - 1\n",
        "        p_mean_list = ((stat.mean * p_n_adj) - data) / p_n_adj  # This value is a list\n",
        "\n",
        "        p_n_sum = np.ma.sum(p_n_adj)\n",
        "        p_weights = np.ma.divide(stat.n, p_n_sum)\n",
        "        p_weighted_mean = p_mean_list * p_weights\n",
        "\n",
        "        p_mean = np.ma.sum(p_weighted_mean)\n",
        "        p_std = np.ma.std(p_weighted_mean)\n",
        "        p_n = np.ma.sum(p_n_adj)\n",
        "        p_se = p_std / np.ma.sqrt(p_n)\n",
        "\n",
        "        # Get the mean difference\n",
        "        mean_diff = i_mean - p_mean\n",
        "        # Standard error difference\n",
        "        se = np.ma.sqrt(i_se**2, p_se**2)\n",
        "\n",
        "        # t test\n",
        "        t = mean_diff / se\n",
        "\n",
        "        # degrees of freedom\n",
        "        df = i_n + p_n - 2\n",
        "\n",
        "        # p value\n",
        "        # p = stats.t.cdf(np.abs(t), df=df) * 2\n",
        "\n",
        "        # Adjust p for FDR\n",
        "\n",
        "        years = list(range(1, len(p_mean_list)))\n",
        "        pop_trend_model = OLS(p_mean_list, years)\n",
        "        pop_trend_result = pop_trend_model.fit()\n",
        "\n",
        "        ind_trend_model = GLSAR(data, years)\n",
        "        ind_trend_result = ind_trend_model.fit()\n",
        "\n",
        "        i = 1\n",
        "\n",
        "    def _t_test_strict_r_logic(self, stat, data):\n",
        "        # Mask out NoData pixels\n",
        "        data = np.ma.masked_where(data < 0, data)\n",
        "        # Get the individual stats\n",
        "        i_mean = np.ma.mean(data)  # This is a single value (mean over time)\n",
        "\n",
        "        # Bail early if there's not data\n",
        "        if np.ma.is_masked(i_mean):\n",
        "            return None\n",
        "\n",
        "        # Nan the missing values (statsmodels doesn't seem to acknowledge masked arrays)\n",
        "        nan_data = data.astype(float).filled(np.nan)\n",
        "        # i_mean_model = GLSAR(nan_data, missing='drop')\n",
        "        # i_mean_result = i_mean_model.fit()\n",
        "        # i_se = i_mean_result.bse\n",
        "\n",
        "        # Adjust population stats to remove point\n",
        "        # The -1 operations are because we removed a data point\n",
        "        p_n_adj = stat.n - 1\n",
        "        p_mean_list = ((stat.mean * p_n_adj) - data) / p_n_adj  # This value is a list\n",
        "        p_mean_list = np.ma.masked_where(p_mean_list < 0, p_mean_list)\n",
        "        # p_mean = np.ma.mean(p_mean_list)  # This is a single value\n",
        "\n",
        "        # Get the mean difference\n",
        "        # mean_diff = i_mean - p_mean\n",
        "\n",
        "        # t test\n",
        "        # t = mean_diff / i_se\n",
        "\n",
        "        # Skip doing the calculations manually, just do a basic t test\n",
        "        t, p = st.ttest_rel(nan_data, p_mean_list, nan_policy=\"omit\")\n",
        "\n",
        "        if np.ma.is_masked(t):\n",
        "            return None\n",
        "\n",
        "        # degrees of freedom\n",
        "        # df = i_mean_result.nobs - len(i_mean_result.params)\n",
        "\n",
        "        # p value\n",
        "        # p = st.t.cdf(np.abs(t), df=df) * 2\n",
        "\n",
        "        # Make years list for regressions\n",
        "        years = np.array(list(range(len(p_mean_list))))\n",
        "\n",
        "        # Nan years where there's missing data\n",
        "        mask_years = np.ma.array(years.astype(float), mask=p_mean_list.mask)\n",
        "        nan_years = mask_years.filled(np.nan)\n",
        "        pop_trend_model = GLSAR(p_mean_list, nan_years, missing=\"drop\")\n",
        "        pop_trend_result = pop_trend_model.fit()\n",
        "\n",
        "        mask_years = np.ma.array(years.astype(float), mask=data.mask)\n",
        "        nan_years = mask_years.filled(np.nan)\n",
        "        ind_trend_model = GLSAR(data, nan_years, missing=\"drop\")\n",
        "        ind_trend_result = ind_trend_model.fit()\n",
        "\n",
        "        pop_slope = pop_trend_result.params[0]\n",
        "        ind_slope = ind_trend_result.params[0]\n",
        "\n",
        "        slope_diff = ind_slope - pop_slope\n",
        "        slope_se = ind_trend_result.bse\n",
        "        # slope_pop_se = pop_trend_result.bse\n",
        "        slope_t = slope_diff / slope_se\n",
        "\n",
        "        # slope_n = len(ind_trend_model.endog)\n",
        "        # slope_pop_n = len(pop_trend_model.endog)\n",
        "\n",
        "        df = ind_trend_result.nobs - len(ind_trend_result.params)\n",
        "\n",
        "        slope_p = st.t.sf(np.abs(slope_t), df=df) * 2\n",
        "\n",
        "        # slope_t, slope_p = st.ttest_ind_from_stats(ind_slope, slope_se, slope_n, pop_slope, slope_pop_se, slope_pop_n, equal_var=False)\n",
        "\n",
        "        # vals = np.array([t[0], p[0], slope_t[0], slope_p[0]])\n",
        "        vals = np.array([t, p, slope_t[0], slope_p[0]])\n",
        "\n",
        "        return vals\n",
        "\n",
        "class Degradation(object):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        # self.zone_raster_path = kwargs['zone_raster']\n",
        "\n",
        "        # TODO check for banded raster vs list of rasters\n",
        "        # self.data_raster_path = kwargs['data_raster']\n",
        "        return\n",
        "\n",
        "    def degradation(self, data):\n",
        "\n",
        "        I = data.shape[0]\n",
        "        J = data.shape[1]\n",
        "        K = data.shape[2]\n",
        "        output = np.empty((I, J, K))\n",
        "\n",
        "        for i in range(I):\n",
        "            for j in range(J):\n",
        "                for k in range(K):\n",
        "                    val = data[i, j, k]\n",
        "                    output[i, j, k] = val\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "XYEyGA_xzElN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zone_name = \"BpsZonRobGb_wgs84_nc\"\n",
        "gcs_degradation_path = \"gs://fuelcast-data/degradation/\"\n",
        "gcs_rpms_path = \"gs://fuelcast-data/rpms/\"\n",
        "\n",
        "zone_raster_path = f\"{gcs_degradation_path}{zone_name}/{zone_name}.tif\"\n",
        "data_raster_path = f\"./data/{zone_name}/rpms_stack.tif\"\n",
        "dummy_path = \"./test.tif\""
      ],
      "metadata": {
        "id": "Ohw4agRL1Ugb"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}